{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(Q, K, V):\n",
    "    '''\n",
    "        Q: B x N x d\n",
    "        K: B x N x d\n",
    "        V: B x N x d\n",
    "    '''\n",
    "    d = Q.shape[-1]\n",
    "\n",
    "    attn_logits = Q @ K.transpose(-2, -1) # B x N x N\n",
    "    scaled_attn_logits = attn_logits / torch.sqrt(torch.tensor(d, dtype=torch.float32))\n",
    "    attn_scores = torch.softmax(scaled_attn_logits, dim=-1) # B x N x N\n",
    "    # take softmax-across the row => cuz row gets multiplied with column of V (so softmax-attn should be multiplied to each elemen in V (V is a column vector))\n",
    "\n",
    "    # B x N x N dot B x N x d => B x N x d\n",
    "    attn = attn_scores @ V\n",
    "\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention(Q, K, V, M):\n",
    "    # break down into blocks (tiling)\n",
    "    # blocks of Q, K and V\n",
    "    \n",
    "    # Calculate Block size based on M (bytes)\n",
    "    d = Q.shape[-1] \n",
    "    N = Q.shape[-2]\n",
    "    d_size = 4*d #(4 bytes for each float)\n",
    "    Br = int(min(math.ceil(M / d_size), d))\n",
    "    Bc = int(math.ceil(M / d_size))\n",
    "    \n",
    "    # init out\n",
    "    # init l\n",
    "    out = torch.zeros_like(Q)\n",
    "    l = torch.zeros(N)\n",
    "    # divde Q into block sizes of Br x d\n",
    "    Tr = int(math.ceil(N / Br))\n",
    "    # divide K, V into block sizes of Bc x d\n",
    "    Tc = int(math.ceil(N / Bc))\n",
    "\n",
    "    for j in range(Tc):\n",
    "        Kj, Vj = K[j*Bc:(j+1)*Bc], V[j*Bc:(j+1)*Bc]\n",
    "        for i in range(Tr):\n",
    "            Qi = Q[i*Br:(i+1)*Br]\n",
    "\n",
    "            Sij = (Qi @ Kj.transpose(-2, -1)) /  torch.sqrt(torch.tensor(d, dtype=torch.float32))\n",
    "            Pij = torch.exp(Sij) # block exp\n",
    "            lij = torch.sum(Pij, dim=-1)\n",
    "                        \n",
    "            # new updated sum\n",
    "            lnew = l[i*Br:(i+1)*Br] + lij\n",
    "            \n",
    "            # previous running sum\n",
    "            li = l[i*Br:(i+1)*Br].unsqueeze(-1)\n",
    "\n",
    "            # print('Pij:', Pij.shape, 'Vj:', Vj.shape, 'Oij:', out[i*Br:(i+1)*Br].shape) # DEBUG\n",
    "\n",
    "            numerator = ((Pij @ Vj) + out[i*Br:(i+1)*Br]*li)\n",
    "\n",
    "            # estimated softmax\n",
    "            out[i*Br:(i+1)*Br] = numerator / lnew.unsqueeze(-1)\n",
    "            \n",
    "            # Update denominator\n",
    "            l[i*Br:(i+1)*Br] = lnew\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # N = 128, d = 64\n",
    "Q = torch.randn(128, 64, dtype=torch.float32)\n",
    "K = torch.randn(128, 64, dtype=torch.float32)\n",
    "V = torch.randn(128, 64, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_attn_out = standard_attention(Q, K, V)\n",
    "fl_out = flash_attention(Q, K, V, M=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0201, -0.1788, -0.0363,  ..., -0.0315,  0.0576,  0.0255],\n",
       "        [ 0.2961, -0.1752, -0.1401,  ..., -0.0553,  0.0707, -0.1922],\n",
       "        [ 0.0422,  0.0714,  0.0121,  ...,  0.0510, -0.1946, -0.0109],\n",
       "        ...,\n",
       "        [ 0.0184, -0.0160, -0.1022,  ..., -0.1712, -0.1380, -0.0208],\n",
       "        [-0.1606, -0.1271,  0.0710,  ..., -0.0724,  0.0618, -0.1609],\n",
       "        [ 0.0693, -0.2189,  0.0981,  ...,  0.1996, -0.0690,  0.0424]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0201, -0.1788, -0.0363,  ..., -0.0315,  0.0576,  0.0255],\n",
       "        [ 0.2961, -0.1752, -0.1401,  ..., -0.0553,  0.0707, -0.1922],\n",
       "        [ 0.0422,  0.0714,  0.0121,  ...,  0.0510, -0.1946, -0.0109],\n",
       "        ...,\n",
       "        [ 0.0184, -0.0160, -0.1022,  ..., -0.1712, -0.1380, -0.0208],\n",
       "        [-0.1606, -0.1271,  0.0710,  ..., -0.0724,  0.0618, -0.1609],\n",
       "        [ 0.0693, -0.2189,  0.0981,  ...,  0.1996, -0.0690,  0.0424]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(fl_out, std_attn_out, rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
